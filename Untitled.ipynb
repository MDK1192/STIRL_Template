{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import der Daten & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benötigte Bibliotheken Importieren\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import re\n",
    "import collections\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten importieren und inspizieren\n",
    "articles = pd.read_csv('Pubmed_Data.csv', sep =None)\n",
    "\n",
    "#print(articles.head())\n",
    "#print(articles.shape)\n",
    "#print(articles.describe())\n",
    "articles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional:\n",
    "#Nicht benötigte Spalten löschen\n",
    "#articles.drop(['day'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template für Vorbereitung der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datentypen anpassen\n",
    "articles[\"year\"] = articles['year'].astype('str')\n",
    "articles[\"month\"] = articles['month'].astype('str')\n",
    "articles[\"day\"] = articles['day'].astype('str')\n",
    "articles[\"decade\"] = articles['day'].astype('str')\n",
    "\n",
    "#Monats und Tagesbezeichnungen mit 0 auffüllen um ein konsistentes Format zu erzeugen\n",
    "for i in range(0,len(articles.day)):\n",
    "    if(len(articles[\"day\"][i]) < 2):        \n",
    "        articles[\"day\"][i] = '0' + articles[\"day\"][i]\n",
    "    if(len(articles[\"month\"][i]) < 2):        \n",
    "        articles[\"month\"][i] = '0' + articles[\"month\"][i]\n",
    "\n",
    "#Jahrzehnt filtern und Spalte erzeugen \n",
    "for i in range(0,len(articles.year)):\n",
    "    articles[\"decade\"][i]=articles[\"year\"][i][0:3]+\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Templates: Worclouds Unigramm & Bigramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entfernen von NAs\n",
    "abstracts_nonNA = articles[articles['abstract'].notna()]\n",
    "\n",
    "#joinen von einzelnen Abstracts\n",
    "long_string_abstracts = ','.join(list(abstracts_nonNA['abstract'].values))\n",
    "\n",
    "#stopwords initieren um einzelne Therme herauszufiltern\n",
    "#stopwords= set(STOPWORDS)\n",
    "\n",
    "#wordcloud erzeugen\n",
    "wordcloud = WordCloud(background_color=\"white\", contour_width=3, contour_color='steelblue', width= 1000, height= 1000)\n",
    "wordcloud.generate(long_string_abstracts)\n",
    "\n",
    "#wordcloud visualisieren\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erstellen von Counts_df\n",
    "counts_no_urls = collections.Counter(long_string_abstracts.split())\n",
    "df_abstracts_counts = pd.DataFrame.from_dict(counts_no_urls, orient='index')\n",
    "df_abstracts_counts.drop(stopwords, inplace=True, errors='ignore')\n",
    "df_abstracts_counts.drop([',','.','(',')',';','%','mast','cell'], inplace=True, errors='ignore')\n",
    "\n",
    "#Erzeugen von Dataframe und generieren von Grafik\n",
    "df_abstracts_counts_unigram = df_abstracts_counts.nlargest(200, 0).iloc[::1]\n",
    "\n",
    "#Speichern der Ergebnisse\n",
    "df_abstracts_counts_unigram.to_csv(\"df_abstracts_counts_unigram.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#altes output.csv entfernen\n",
    "if os.path.exists(\"output.csv\"):\n",
    "    os.remove(\"output.csv\") \n",
    "\n",
    "#Daten transformieren\n",
    "data = articles['abstract'].values.tolist()\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    data[i] = str(data[i]).replace(\",\", \"\")\n",
    "    data[i] = str(data[i]).replace(\"%\", \"\")\n",
    "    data[i] = str(data[i]).replace(\"&\", \"\")\n",
    "    data[i] = str(data[i]).replace(\";\", \"\")\n",
    "    data[i] = str(data[i]).replace(\".\", \"\")\n",
    "    data[i] = str(data[i]).replace(\":\", \"\")\n",
    "    data[i] = str(data[i]).replace(\"/\", \"\")\n",
    "    data[i] = str(data[i]).replace(\"(\", \"\")\n",
    "    data[i] = str(data[i]).replace(\")\", \"\")\n",
    "    data[i] = str(data[i]).replace(\"[\", \"\")\n",
    "    data[i] = str(data[i]).replace(\"]\", \"\")\n",
    "    data[i] = str(data[i]).replace(\"-\", \"\")\n",
    "    bigrm = list(nltk.bigrams(str(data[i]).split()))\n",
    "\n",
    "    with open(\"output.csv\", \"a\") as f:\n",
    "        print(*map('_'.join, bigrm), sep=' ', file=f)\n",
    "        \n",
    "with open('output.csv', newline='', encoding=\"utf8\", errors='ignore') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data_bigram = list(reader)\n",
    "\n",
    "data_bigram=sum(data_bigram, [])\n",
    "\n",
    "#joinen von einzelnen Abstracts\n",
    "data_bigram = ','.join(list(data_bigram))\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\", contour_width=3, contour_color='steelblue', width= 1000, height= 1000)\n",
    "wordcloud.generate(data_bigram)\n",
    "\n",
    "#wordcloud visualisieren\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erstellen von Counts_df\n",
    "counts_no_urls = collections.Counter(data_bigram.split())\n",
    "df_abstracts_counts = pd.DataFrame.from_dict(counts_no_urls, orient='index')\n",
    "df_abstracts_counts.drop(stopwords, inplace=True, errors='ignore')\n",
    "df_abstracts_counts.drop([',','.','(',')',';','%','mast','cell'], inplace=True, errors='ignore')\n",
    "\n",
    "#Erzeugen von Dataframe und generieren von Grafik\n",
    "df_abstracts_counts_bigram = df_abstracts_counts.nlargest(200, 0).iloc[::1]\n",
    "\n",
    "#alte datei entfernen\n",
    "if os.path.exists(\"df_abstracts_counts_bigram.csv\"):\n",
    "    os.remove(\"df_abstracts_counts_bigram.csv\") \n",
    "\n",
    "#Speichern der Ergebnisse\n",
    "df_abstracts_counts_bigram.to_csv(\"df_abstracts_counts_bigram.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Overview Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keywords und MeSH Codes von Dataframe ableiten und bereinigen\n",
    "keywords = articles.keywords\n",
    "\n",
    "keywords = keywords.str.split(';')\n",
    "articles.drop(['keywords'], axis=1)\n",
    "\n",
    "for i in range(0,keywords.size):\n",
    "    if(keywords.isna()[i]==False):\n",
    "        for j in range(0,len(keywords[i])):\n",
    "            if(j!=0):\n",
    "                keywords[i][j]= keywords[i][j][1:]\n",
    "        keywords[i] = [s.replace(\";\", \"\") for s in keywords[i]]\n",
    "        #keywords[i] = [s.replace(\" \", \"\") for s in keywords[i]]\n",
    "        articles.keywords[i] = articles.keywords[i].lower()\n",
    "        \n",
    "#Keywords in lowercase transformieren & duplicate entfernen\n",
    "keywords_total = []\n",
    "for i in range(0,keywords.size):\n",
    "    if(keywords.isna()[i]==False):\n",
    "        keywords[i] = [x.lower() for x in keywords[i]]\n",
    "        keywords[i] = keywords[i] = list(set(keywords[i])) \n",
    "    keywords_total.append(keywords[[i]])\n",
    "    \n",
    "#keywords transformieren um mit wordcloud kompatibel zu sein\n",
    "flat_list_keywords = []\n",
    "for sublist in keywords_total:\n",
    "    for item in sublist:\n",
    "        try: \n",
    "            for element in item:\n",
    "                flat_list_keywords.append(element)\n",
    "        except:\n",
    "            flat_list_keywords.append(item)\n",
    "            \n",
    "flat_list_keywords = [x for x in flat_list_keywords if str(x) != 'nan']\n",
    "\n",
    "#Validieren und entfernen von \" \"\n",
    "for i in range(0,len(flat_list_keywords)):\n",
    "    flat_list_keywords[i] = flat_list_keywords[i].replace(',','')\n",
    "    flat_list_keywords[i] = flat_list_keywords[i].replace('\"','')\n",
    "    if flat_list_keywords[i].startswith(\" \", 0):\n",
    "            flat_list_keywords[i] = flat_list_keywords[i][1:]\n",
    "                        \n",
    "#duplikate entfernen\n",
    "flat_list_keywords = flat_list_keywords = list(set(flat_list_keywords))\n",
    "len(flat_list_keywords)\n",
    "\n",
    "#dataframe für keyword historie generieren\n",
    "keywords_df = pd.DataFrame(index=set(flat_list_keywords), columns=pd.Series(range(int(str(articles.year.min())[:4]),int(str(articles.year.max())[:4])+1)))\n",
    "\n",
    "for col in keywords_df.columns:\n",
    "    keywords_df[col].values[:] = 0\n",
    "    \n",
    "#Keywords dataframe mit Keyword historie ausfüllen !!Benötigt mehrere Stunden!!\n",
    "def keyword_function(row_keyword):\n",
    "    articles.apply(lambda row_articles: articles_function(row_keyword, row_articles), axis=1)\n",
    "    \n",
    "def articles_function(row_keyword, row_articles):\n",
    "    if(row_articles.isna()[\"keywords\"]==False):\n",
    "        try: \n",
    "            temp_row_articles= row_articles[\"keywords\"].split('; ')\n",
    "        except: \n",
    "            pass\n",
    "        if(row_keyword.name in temp_row_articles):\n",
    "            try: \n",
    "                temp_result = list(filter(lambda x: x == row_keyword.name, temp_row_articles))\n",
    "            except: \n",
    "                pass\n",
    "            if(row_keyword.name == temp_result[0]): \n",
    "                try:\n",
    "                    #print(str(row_articles[11])[:4])\n",
    "                    row_keyword[row_keyword.index[list(row_keyword.index).index(int(str(row_articles['year'])[:4]))]] = row_keyword[row_keyword.index[list(row_keyword.index).index(int(str(row_articles['year'])[:4]))]] + 1              \n",
    "                except: \n",
    "                    pass\n",
    "keywords_df.apply(lambda row_keyword: keyword_function(row_keyword), axis=1)\n",
    "\n",
    "#Erzeugen von Summenspalte\n",
    "keywords_df[\"sum\"] = keywords_df.sum(axis=1)\n",
    "\n",
    "#Speichern der Ergebnisse\n",
    "keywords_df.to_csv(\"keywords_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords history loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Laden von fertigen Keywords Dataframe\n",
    "keywords_df=pd.read_csv(\"keywords_df.csv\")\n",
    "keywords_df=keywords_df.set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifizieren der am häufigsten vorkommenden Therme anhand von Summenspalte\n",
    "[p for p in keywords_df.nlargest(300, 'sum').index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erstellen einer Liste aus den am häufigsten vorkommenden Thermen\n",
    "#list_to_delete=[]\n",
    "\n",
    "#for element in list_to_delete:\n",
    "#    keywords_df.drop(element, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Betrachten von fertigen Keywords_df\n",
    "keywords_df=keywords_df[keywords_df.index.notnull()]\n",
    "keywords_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Forschungsfragen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Was sind die primären Tagwords, die in den Fachartikeln vorkommen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erzeugen von Dataframe und generieren von Grafik\n",
    "keywords_filtered = keywords_df.nlargest(30, 'sum').iloc[::-1]\n",
    "sns.set(rc = {'figure.figsize':(15,15)})\n",
    "ax = sns.barplot(x=\"sum\", y= keywords_filtered.index, data=keywords_filtered, order=keywords_filtered['sum'].index[::-1], palette=\"Blues_d\")\n",
    "#ax.axes.set_title(\"Übersicht über die häufigsten Keywords und MeSH-Codes\",fontsize=30)\n",
    "ax.set_xlabel(\"Anzahl\",fontsize=20)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=20)\n",
    "ax.set_ylabel(\"Keywords & MeSH Codes\",fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Speichern der Ergebnisse\n",
    "keywords_filtered = keywords_df.nlargest(30000, 'sum').iloc[::1]\n",
    "keywords_filtered.to_csv(\"keywords_filtered_Forschungsfrage1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lassen sich über Jahrzehnte hinweg in den Tagwords wesentliche Entwicklungen erkennen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erzeugen von weiteren Dataframes\n",
    "keywords_df_60 = keywords_df.iloc[:,0:5]\n",
    "keywords_df_60[\"sum\"] = keywords_df_60.sum(axis=1)\n",
    "keywords_df[\"60s_sum\"] = keywords_df_60[\"sum\"]\n",
    "\n",
    "keywords_df_70 = keywords_df.iloc[:,5:15]\n",
    "keywords_df_70[\"sum\"] = keywords_df_70.sum(axis=1)\n",
    "keywords_df[\"70s_sum\"] = keywords_df_70[\"sum\"]\n",
    "\n",
    "keywords_df_80 = keywords_df.iloc[:,15:25]\n",
    "keywords_df_80[\"sum\"] = keywords_df_80.sum(axis=1)\n",
    "keywords_df[\"80s_sum\"] = keywords_df_80[\"sum\"]\n",
    "\n",
    "keywords_df_90 = keywords_df.iloc[:,25:35]\n",
    "keywords_df_90[\"sum\"] = keywords_df_90.sum(axis=1)\n",
    "keywords_df[\"90s_sum\"] = keywords_df_90[\"sum\"]\n",
    "\n",
    "keywords_df_00 = keywords_df.iloc[:,35:45]\n",
    "keywords_df_00[\"sum\"] = keywords_df_00.sum(axis=1)\n",
    "keywords_df[\"00s_sum\"] = keywords_df_00[\"sum\"]\n",
    "\n",
    "keywords_df_10 = keywords_df.iloc[:,45:56]\n",
    "keywords_df_10[\"sum\"] = keywords_df_10.sum(axis=1)\n",
    "keywords_df[\"10s_sum\"] = keywords_df_10[\"sum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generieren von Grafiken\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "fig.subplots_adjust(hspace=1.4, wspace=0.1)\n",
    "ax = fig.add_subplot(3, 1, 1)                     \n",
    "keywords_df_60 = keywords_df_60.nlargest(20, 'sum').iloc[::-1]\n",
    "sns.barplot(y=\"sum\", x= keywords_df_60.index, data=keywords_df_60, order=keywords_df_60['sum'].index[::-1], palette=\"Blues_d\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=15)\n",
    "ax.set_ylabel(\"1965-1969\",fontsize=20)\n",
    "ax.set_xlabel('')\n",
    "\n",
    "ax = fig.add_subplot(3, 1, 2)                     \n",
    "keywords_df_80 = keywords_df_80.nlargest(20, 'sum').iloc[::-1]\n",
    "sns.barplot(y=\"sum\", x= keywords_df_80.index, data=keywords_df_80, order=keywords_df_80['sum'].index[::-1], palette=\"Blues_d\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=15)\n",
    "ax.set_ylabel(\"1980-1989\",fontsize=20)\n",
    "ax.set_xlabel('')\n",
    "\n",
    "ax = fig.add_subplot(3, 1, 3)                     \n",
    "keywords_df_70 = keywords_df_70.nlargest(20, 'sum').iloc[::-1]\n",
    "sns.barplot(y=\"sum\", x= keywords_df_70.index, data=keywords_df_70, order=keywords_df_70['sum'].index[::-1], palette=\"Blues_d\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=15)\n",
    "ax.set_ylabel(\"1970-1979\",fontsize=20)\n",
    "ax.set_xlabel('')\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generieren von Grafiken\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "fig.subplots_adjust(hspace=1, wspace=0.1)\n",
    "ax = fig.add_subplot(3, 1, 1)                     \n",
    "keywords_df_90 = keywords_df_90.nlargest(20, 'sum').iloc[::-1]\n",
    "sns.barplot(y=\"sum\", x= keywords_df_90.index, data=keywords_df_90, order=keywords_df_90['sum'].index[::-1], palette=\"Blues_d\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=15)\n",
    "ax.set_ylabel(\"1990-1999\",fontsize=20)  \n",
    "ax.set_xlabel('')\n",
    "\n",
    "ax = fig.add_subplot(3, 1, 2)                     \n",
    "keywords_df_00 = keywords_df_00.nlargest(20, 'sum').iloc[::-1]\n",
    "sns.barplot(y=\"sum\", x= keywords_df_00.index, data=keywords_df_00, order=keywords_df_00['sum'].index[::-1], palette=\"Blues_d\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=15)\n",
    "ax.set_ylabel(\"2000-2009\",fontsize=20)\n",
    "ax.set_xlabel('')\n",
    "\n",
    "ax = fig.add_subplot(3, 1, 3)                     \n",
    "keywords_df_10 = keywords_df_10.nlargest(20, 'sum').iloc[::-1]\n",
    "sns.barplot(y=\"sum\", x= keywords_df_10.index, data=keywords_df_10, order=keywords_df_10['sum'].index[::-1], palette=\"Blues_d\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=15)\n",
    "ax.set_ylabel(\"2010-2021\",fontsize=20)\n",
    "ax.set_xlabel('')\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Existieren Themenfelder, die gerade stark ansteigen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifizieren von relevanten Begriffe anhand Verhältnis von Publikationen\n",
    "keywords_filtered = keywords_df[(keywords_df['00s_sum'] > 5) & (keywords_df['00s_sum'] > 0)]\n",
    "keywords_filtered['increase_quotient'] = keywords_filtered['10s_sum'] / keywords_filtered['00s_sum']\n",
    "keywords_filtered = keywords_filtered.nlargest(20, \"increase_quotient\")\n",
    "keywords_filtered = keywords_filtered.iloc[:,58:65]\n",
    "\n",
    "keywords_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Speichern der Ergebnisse\n",
    "keywords_df_60 = keywords_df_60.nlargest(10000, 'sum').iloc[::1]\n",
    "keywords_df_70 = keywords_df_70.nlargest(10000, 'sum').iloc[::1]\n",
    "keywords_df_80 = keywords_df_80.nlargest(10000, 'sum').iloc[::1]\n",
    "keywords_df_90 = keywords_df_90.nlargest(10000, 'sum').iloc[::1]\n",
    "keywords_df_00 = keywords_df_00.nlargest(10000, 'sum').iloc[::1]\n",
    "keywords_df_10 = keywords_df_10.nlargest(10000, 'sum').iloc[::1]\n",
    "keywords_filtered_trend_pos = keywords_df[(keywords_df['60s_sum'] < keywords_df['70s_sum']) & (keywords_df['80s_sum'] < keywords_df['90s_sum']) &(keywords_df['00s_sum'] < keywords_df['10s_sum'])].iloc[::1]\n",
    "keywords_filtered_trend_neg = keywords_df[(keywords_df['10s_sum'] < keywords_df['80s_sum']) & (keywords_df['80s_sum'] < keywords_df['60s_sum'])].iloc[::1]\n",
    "\n",
    "keywords_filtered = keywords_df[(keywords_df['00s_sum'] > 5) & (keywords_df['00s_sum'] > 0) & (keywords_df['00s_sum'] > 0)]\n",
    "keywords_filtered['increase_quotient'] = keywords_filtered['10s_sum'] / keywords_filtered['00s_sum']\n",
    "keywords_filtered = keywords_filtered.nlargest(20000, \"increase_quotient\")\n",
    "keywords_filtered_newcomers = keywords_filtered.iloc[:,58:65]\n",
    "\n",
    "keywords_df_60.to_csv(\"keywords_df_60_Forschungsfrage2.csv\")\n",
    "keywords_df_70.to_csv(\"keywords_df_70_Forschungsfrage2.csv\")\n",
    "keywords_df_80.to_csv(\"keywords_df_80_Forschungsfrage2.csv\")\n",
    "keywords_df_90.to_csv(\"keywords_df_90_Forschungsfrage2.csv\")\n",
    "keywords_df_00.to_csv(\"keywords_df_00_Forschungsfrage2.csv\")\n",
    "keywords_df_10.to_csv(\"keywords_df_10_Forschungsfrage2.csv\")\n",
    "keywords_filtered_newcomers.to_csv(\"keywords_filtered_trend_newcomer_Forschungsfrage2.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beispiel: Welche Krankheiten, Symptome & Diagnosen werden primär in den Artikeln beschrieben, welche eher selten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importieren von MeSH-Codes\n",
    "mesh_codes = pd.read_csv(\"MeSH-Codes.csv\", sep=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtern der MeSH-Codes auf bestimmte Terme\n",
    "mesh_codes_diseases = mesh_codes[mesh_codes.TreeNumberList.str.contains(\"C0|C1|C2\", na=False)]\n",
    "mesh_codes_diseases = mesh_codes_diseases.DescriptorName.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erzeugen von Dataframe und generieren von Grafik anhand von MeSH-Codes und Keywordfs_df\n",
    "keywords_filtered = keywords_df[keywords_df.index.str.contains('|'.join(mesh_codes_diseases), na=False)].nlargest(30, 'sum').iloc[::-1]\n",
    "sns.set(rc = {'figure.figsize':(15,15)})\n",
    "ax = sns.barplot(x=\"sum\", y= keywords_filtered.index, data=keywords_filtered, order=keywords_filtered['sum'].index[::-1], palette=\"Blues_d\")\n",
    "#ax.axes.set_title(\"Übersicht über die häufigsten Keywords und MeSH-Codes\",fontsize=50)\n",
    "ax.set_yticklabels(ax.get_yticklabels(),fontsize=20)\n",
    "ax.set_xlabel(\"Anzahl\",fontsize=20)\n",
    "ax.set_ylabel(\"Keywords & MeSH Codes\",fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifizieren von relevanten Begriffe anhand Verhältnis von Publikationen].iloc[::-1]\n",
    "keywords_filtered = keywords_filtered[(keywords_filtered['00s_sum'] > 5) & (keywords_filtered['00s_sum'] > 0)]\n",
    "keywords_filtered['increase_quotient'] = keywords_filtered['10s_sum'] / keywords_filtered['00s_sum']\n",
    "keywords_filtered = keywords_filtered.nlargest(50, \"increase_quotient\")\n",
    "keywords_filtered = keywords_filtered.iloc[:,58:65]\n",
    "keywords_filtered.to_csv(\"keywords_filtered_trend_newcomer_Forschungsfrage3_diseases.csv\")\n",
    "\n",
    "keywords_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anzeigen von Grafiken für Überprüfung von Entiwcklung der Begriffe\n",
    "mesh_codes_diseases = mesh_codes[mesh_codes.TreeNumberList.str.contains(\"C0|C1|C2\", na=False)]\n",
    "mesh_codes_diseases = mesh_codes_diseases.DescriptorName.str.lower()\n",
    "keywords_filtered = keywords_df[keywords_df.index.str.contains('|'.join(mesh_codes_diseases), na=False)].nlargest(30000, 'sum').iloc[::1]\n",
    "keywords_filtered\n",
    "keywords_filtered = keywords_filtered.transpose()\n",
    "keywords_filtered.drop(index=['sum', '60s_sum', '70s_sum','80s_sum','90s_sum','00s_sum', '10s_sum'], inplace=True)\n",
    "keywords_filtered = keywords_filtered[[\"inflammation\",\"asthma\",\"hypersensitivity\", \"anaphylaxis\", \"mast-cell sarcoma\", \"mastocytosis\", \"dog diseases\", \"urticaria\", \"skin neoplasms\", \"urticaria pigmentosa\", \"neoplasms\", \"rhinitis\"]]\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.1)\n",
    "ax = fig.add_subplot(6, 2, 1)\n",
    "sns.barplot(x=keywords_filtered.index,y=keywords_filtered.iloc[:,0],data=keywords_filtered, palette=\"Blues_d\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=12)\n",
    "ax.set_ylabel(keywords_filtered.iloc[:,0].name,fontsize = 15)\n",
    "ax = fig.add_subplot(6, 2, 2)\n",
    "sns.barplot(x=keywords_filtered.index,y=keywords_filtered.iloc[:,1],data=keywords_filtered, palette=\"Blues_d\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=12)\n",
    "ax.set_ylabel(keywords_filtered.iloc[:,1].name,fontsize = 15)\n",
    "ax = fig.add_subplot(6, 2, 3)\n",
    "sns.barplot(x=keywords_filtered.index,y=keywords_filtered.iloc[:,2],data=keywords_filtered, palette=\"Blues_d\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=12)\n",
    "ax.set_ylabel(keywords_filtered.iloc[:,2].name,fontsize = 15)\n",
    "ax = fig.add_subplot(6, 2, 4)\n",
    "sns.barplot(x=keywords_filtered.index,y=keywords_filtered.iloc[:,3],data=keywords_filtered, palette=\"Blues_d\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=12)\n",
    "ax.set_ylabel(keywords_filtered.iloc[:,3].name,fontsize = 15)\n",
    "ax = fig.add_subplot(6, 2, 5)\n",
    "sns.barplot(x=keywords_filtered.index,y=keywords_filtered.iloc[:,4],data=keywords_filtered, palette=\"Blues_d\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=12)\n",
    "ax.set_ylabel(keywords_filtered.iloc[:,4].name,fontsize = 15)\n",
    "ax = fig.add_subplot(6, 2, 6)\n",
    "sns.barplot(x=keywords_filtered.index,y=keywords_filtered.iloc[:,5],data=keywords_filtered, palette=\"Blues_d\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=12)\n",
    "ax.set_ylabel(keywords_filtered.iloc[:,5].name,fontsize = 15)\n",
    "ax = fig.add_subplot(6, 2, 7)\n",
    "sns.barplot(x=keywords_filtered.index,y=keywords_filtered.iloc[:,6],data=keywords_filtered, palette=\"Blues_d\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=12)\n",
    "ax.set_ylabel(keywords_filtered.iloc[:,6].name,fontsize = 15)\n",
    "ax = fig.add_subplot(6, 2, 8)\n",
    "sns.barplot(x=keywords_filtered.index,y=keywords_filtered.iloc[:,7],data=keywords_filtered, palette=\"Blues_d\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=12)\n",
    "ax.set_ylabel(keywords_filtered.iloc[:,7].name,fontsize = 15)\n",
    "ax = fig.add_subplot(6, 2, 9)\n",
    "sns.barplot(x=keywords_filtered.index,y=keywords_filtered.iloc[:,8],data=keywords_filtered, palette=\"Blues_d\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=12)\n",
    "ax.set_ylabel(keywords_filtered.iloc[:,8].name,fontsize = 15)\n",
    "ax = fig.add_subplot(6, 2, 10)\n",
    "sns.barplot(x=keywords_filtered.index,y=keywords_filtered.iloc[:,9],data=keywords_filtered, palette=\"Blues_d\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=12)\n",
    "ax.set_ylabel(keywords_filtered.iloc[:,9].name,fontsize = 15)\n",
    "ax = fig.add_subplot(6, 2, 11)\n",
    "sns.barplot(x=keywords_filtered.index,y=keywords_filtered.iloc[:,10],data=keywords_filtered, palette=\"Blues_d\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=12)\n",
    "ax.set_ylabel(keywords_filtered.iloc[:,10].name,fontsize = 15)\n",
    "ax = fig.add_subplot(6, 2, 12)\n",
    "sns.barplot(x=keywords_filtered.index,y=keywords_filtered.iloc[:,11],data=keywords_filtered, palette=\"Blues_d\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90,fontsize=12)\n",
    "ax.set_ylabel(keywords_filtered.iloc[:,11].name,fontsize = 15)\n",
    "plt.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Speichern der Ergebnisse\n",
    "keywords_filtered = keywords_df[keywords_df.index.str.contains('|'.join(mesh_codes_diseases), na=False)].nlargest(30000, 'sum').iloc[::1]\n",
    "keywords_filtered.to_csv(\"keywords_filtered_Forschungsfrage3_Diseases.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lassen sich durch den Einsatz von Topic Modeling Themengruppen innerhalb des Datenkorpus zum Thema Mastzellen identifizieren?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementieren von Support funktionen für Topic Modeling\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entfernen von NAs\n",
    "abstracts_nonNA = articles[articles['abstract'].notna()]\n",
    "\n",
    "#joinen von einzelnen Abstracts\n",
    "long_string_abstracts = ','.join(list(abstracts_nonNA['abstract'].values))\n",
    "counts_no_urls = collections.Counter(long_string_abstracts.split())\n",
    "df_abstracts_counts = pd.DataFrame.from_dict(counts_no_urls, orient='index')\n",
    "[p for p in df_abstracts_counts.nlargest(400, 0).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import von stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "#stopwords erweitern mit custom liste\n",
    "stop_words.extend([\n",
    "    ',',\n",
    " 'the',\n",
    " 'of',\n",
    " '.',\n",
    " 'and',\n",
    " 'in',\n",
    " 'cell',\n",
    " ')',\n",
    " '(',\n",
    " 'a',\n",
    " 'to',\n",
    " 'mast',\n",
    " 'with',\n",
    " 'by',\n",
    " 'wa',\n",
    " 'that',\n",
    " 'were',\n",
    " 'is',\n",
    " 'for',\n",
    " 'activ',\n",
    " 'from',\n",
    " 'on',\n",
    " 'or',\n",
    " 'are',\n",
    " 'this',\n",
    " 'an',\n",
    " 'we',\n",
    " 'these',\n",
    " 'studi',\n",
    " 'be',\n",
    " 'increas',\n",
    " #'mous',\n",
    " 'not',\n",
    " 'effect',\n",
    " 'respons',\n",
    " 'express',\n",
    " 'releas',\n",
    " 'patient',\n",
    " #'histamin',\n",
    " ';',\n",
    " 'use',\n",
    " 'at',\n",
    " 'it',\n",
    " 'signific',\n",
    " 'il-',\n",
    " 'which',\n",
    " '%',\n",
    " 'have',\n",
    " 'inhibit',\n",
    " 'receptor',\n",
    " 'but',\n",
    " 'also',\n",
    " 'result',\n",
    " 'induc',\n",
    " #'rat',\n",
    " 'after',\n",
    " 'diseas',\n",
    " 'level',\n",
    " 'may',\n",
    " 'human',\n",
    " 'role',\n",
    " 'p',\n",
    " 'tissu',\n",
    " 'immun',\n",
    " 'been',\n",
    " 'both',\n",
    " 'protein',\n",
    " 'suggest',\n",
    " 'mediat',\n",
    " 'treatment',\n",
    " #'inflammatori',\n",
    " 'show',\n",
    " #'allerg',\n",
    " #'skin',\n",
    " '-',\n",
    " 'number',\n",
    " 'ha',\n",
    " 'ige',\n",
    " 'group',\n",
    " 'factor',\n",
    " 'control',\n",
    " 'function',\n",
    " 'than',\n",
    " 'differ',\n",
    " 'degranul',\n",
    " 'their',\n",
    " 'develop',\n",
    " 'reaction',\n",
    " 'between',\n",
    " 'system',\n",
    " 'inflamm',\n",
    " 'mechan',\n",
    " 'associ',\n",
    " 'product',\n",
    " 'present',\n",
    " #'tumor',\n",
    " 'can',\n",
    " 'other',\n",
    " 'compar',\n",
    " 't',\n",
    " 'eosinophil',\n",
    " 'observ',\n",
    " 'no',\n",
    " 'cytokin',\n",
    " 'found',\n",
    " 'involv',\n",
    " 'includ',\n",
    " 'stimul',\n",
    " 'investig',\n",
    " 'type',\n",
    " 'model',\n",
    " 'howev',\n",
    " 'such',\n",
    " '&',\n",
    " '=',\n",
    " 'sever',\n",
    " 'demonstr',\n",
    " 'reduc',\n",
    " 'clinic',\n",
    " 'indic',\n",
    " 'specif',\n",
    " 'dure',\n",
    " #'asthma',\n",
    " 'decreas',\n",
    " 'import',\n",
    " 'signal',\n",
    " 'addit',\n",
    " 'caus',\n",
    " 'normal',\n",
    " 'more',\n",
    " 'had',\n",
    " 'high',\n",
    " 'tryptas',\n",
    " 'antibodi',\n",
    " 'all',\n",
    " 'chang',\n",
    " 'day',\n",
    " ':',\n",
    " 'gene',\n",
    " 'well',\n",
    " 'concentr',\n",
    " 'macrophag',\n",
    " 'into',\n",
    " 'h',\n",
    " 'cd',\n",
    " 'mcs',\n",
    " 'antigen',\n",
    " 'mc',\n",
    " 'regul',\n",
    " #'airway',\n",
    " 'basophil',\n",
    " 'two',\n",
    " 'infect',\n",
    " 'onli',\n",
    " 'examin',\n",
    " 'sensit',\n",
    " 'serum',\n",
    " #'lung',\n",
    " 'our',\n",
    " 'lt',\n",
    " 'secret',\n",
    " 'potenti',\n",
    " '.,the',\n",
    " 'when',\n",
    " 'infiltr',\n",
    " 'analysi',\n",
    " 'inhibitor',\n",
    " '--',\n",
    " 'there',\n",
    " 'determin',\n",
    " 'chronic',\n",
    " 'challeng',\n",
    " 'case',\n",
    " 'blood',\n",
    " 'most',\n",
    " 'report',\n",
    " 'granul',\n",
    " 'acid',\n",
    " 'follow',\n",
    " 'vitro',\n",
    " 'find',\n",
    " 'growth',\n",
    " 'compound',\n",
    " 'play',\n",
    " '+',\n",
    " 'process',\n",
    " 'identifi',\n",
    " 'cultur',\n",
    " 'evalu',\n",
    " 'presenc',\n",
    " 'test',\n",
    " 'detect',\n",
    " 'produc',\n",
    " 'data',\n",
    " 'relat',\n",
    " 'did',\n",
    " 'stain',\n",
    " 'through',\n",
    " 'could',\n",
    " 'bone',\n",
    " 'membran',\n",
    " 'measur',\n",
    " 'b',\n",
    " 'lesion',\n",
    " 'similar',\n",
    " 'lymphocyt',\n",
    " 'one',\n",
    " 'bind',\n",
    " #'intestin',\n",
    " 'pathway',\n",
    " 'neutrophil',\n",
    " 'anim',\n",
    " 'higher',\n",
    " 'enhanc',\n",
    " 'vivo',\n",
    " 'correl',\n",
    " 'symptom',\n",
    " 'kinas',\n",
    " 'local',\n",
    " 'target',\n",
    " 'posit',\n",
    " '+/-',\n",
    " 'drug',\n",
    " 'contain',\n",
    " 'respect',\n",
    " 'reveal',\n",
    " 'interact',\n",
    " 'time',\n",
    " ']',\n",
    " #'allergen',\n",
    " '[',\n",
    " 'evid',\n",
    " 'inject',\n",
    " 'provid',\n",
    " 'recent',\n",
    " '/',\n",
    " 'they',\n",
    " 'differenti',\n",
    " 'contribut',\n",
    " 'suppress',\n",
    " 'treat',\n",
    " 'character',\n",
    " 'some',\n",
    " 'occur',\n",
    " 'site',\n",
    " 'line',\n",
    " #'mastocytosi',\n",
    " 'within',\n",
    " 'plasma',\n",
    " 'subject',\n",
    " 'affect',\n",
    " 'prolifer',\n",
    " 'against',\n",
    " '``',\n",
    " 'periton',\n",
    " 'possibl',\n",
    " 'appear',\n",
    " 'c',\n",
    " 'those',\n",
    " 'condit',\n",
    " 'thus',\n",
    " 'various',\n",
    " 'th',\n",
    " 'direct',\n",
    " 'cellular',\n",
    " 'mucos',\n",
    " 'further',\n",
    " 'anaphylaxi',\n",
    " 'therapeut',\n",
    "# 'nasal',\n",
    " 'shown',\n",
    "# 'allergi',\n",
    " 'molecul',\n",
    " 'although',\n",
    " 'major',\n",
    " 'nan',\n",
    " 'blue',   \n",
    " 'cutan',\n",
    " 'ml',\n",
    " 'mg',\n",
    " 'min',\n",
    " 'ca',   \n",
    " 'wherea',\n",
    " 'while',\n",
    " 'generat',\n",
    " 'common',\n",
    " 'year',   \n",
    " 'therapi',\n",
    " 'lead',\n",
    " 'mrna',\n",
    " 'depend',\n",
    " 'known',\n",
    " 'sm',   \n",
    " 'surfac',\n",
    " 'form',\n",
    " 'under',\n",
    " 'previous',\n",
    " 'first',\n",
    " 'new',\n",
    " 'peptid',\n",
    " 'i',\n",
    " 'disord',\n",
    " 'mani',   \n",
    " 'antagonist',\n",
    "# 'nerv',\n",
    " 'isol',\n",
    " 'le',\n",
    " 'complex',\n",
    " 'acut',\n",
    " 'week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#daten erzeugen und transformieren\n",
    "data = articles['abstract'].values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "#stopwords entfernen\n",
    "data_words_nostops = remove_stopwords(data_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary aus stopwords erzeugen\n",
    "id2word = corpora.Dictionary(data_words_nostops)\n",
    "\n",
    "#textkorpus erzeugen\n",
    "texts = data_words_nostops\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unterstützungsfunktion für Übersicht über Qualität der Modelle erstellen\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics,\n",
    "                                       random_state=42,\n",
    "                                       iterations=10,\n",
    "                                       chunksize=10000, \n",
    "                                       passes=50)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ideale anzahl der topics basierend auf Coherence ermitteln\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_words_nostops, start=2, limit=40, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generieren von Grafik\n",
    "limit=40; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "sns.set(rc = {'figure.figsize':(15,15)})\n",
    "ax = sns.lineplot(x= x, y=coherence_values)\n",
    "ax.set_xlabel(\"Anzahl der Topics\",fontsize=20)\n",
    "ax.set_ylabel(\"Kohärenz\",fontsize=20)\n",
    "#plt.legend(title=\"Topic\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definieren von number von topics für lda\n",
    "num_topics = 9\n",
    "\n",
    "#lda model erzeugen\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics,\n",
    "                                       random_state=42,\n",
    "                                       iterations=10,\n",
    "                                       chunksize=10000, \n",
    "                                       passes=50)\n",
    "\n",
    "#ausgeben der keywords für die gefundenen topics\n",
    "pprint(lda_model.show_topics(num_words = 10, num_topics=num_topics))\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topics visualisieren\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(num_topics))\n",
    "\n",
    "#Falls vorhanden auf existierende daten zugreifen\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, lda_model.id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bestes lda model wählen\n",
    "optimal_model = lda_model\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words = 15, num_topics=num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unterstüzungsfunktion erstellen und Dominates Topic für Dokumente ermitteln\n",
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "df_dominant_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl der Dokumente für jedes Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Prozentualer Wert der Dokumente für jedes Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Anzahl und Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Abändern der Spaltennamen\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untersuchung der Themengruppen auf Trends/Evolution durchgeführt werden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([articles, df_dominant_topics], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv(\"df_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.read_csv(\"df_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.Dominant_Topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erzeugen von Dataframe und generieren von Grafik\n",
    "sns.set(rc = {'figure.figsize':(15,15)})\n",
    "ax = sns.countplot(x=\"Dominant_Topic\", data=df_combined, palette=\"Blues_d\")\n",
    "#ax.axes.set_title(\"Übersicht über die generelle Frequenz von Topics\",fontsize=30)\n",
    "ax.set_xlabel(\"Topic\",fontsize=20)\n",
    "ax.set_ylabel(\"Anzahl\",fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_decade = df_combined.groupby([\"year\",\"Dominant_Topic\"]).size().reset_index()\n",
    "data_by_decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prozentuale Übersicht: Grafik\n",
    "plt.figure(figsize=(14, 14))\n",
    "\n",
    "data_by_decade = df_combined.groupby([\"decade\"]).size().reset_index()\n",
    "data_by_decade_0 = df_combined[df_combined.Dominant_Topic==0.0].groupby([\"decade\",\"Dominant_Topic\"]).size().reset_index()\n",
    "data_by_decade_1 = df_combined[df_combined.Dominant_Topic==1.0].groupby([\"decade\",\"Dominant_Topic\"]).size().reset_index()\n",
    "data_by_decade_2 = df_combined[df_combined.Dominant_Topic==2.0].groupby([\"decade\",\"Dominant_Topic\"]).size().reset_index()\n",
    "data_by_decade_3 = df_combined[df_combined.Dominant_Topic==3.0].groupby([\"decade\",\"Dominant_Topic\"]).size().reset_index()\n",
    "data_by_decade_4 = df_combined[df_combined.Dominant_Topic==4.0].groupby([\"decade\",\"Dominant_Topic\"]).size().reset_index()\n",
    "data_by_decade_5 = df_combined[df_combined.Dominant_Topic==5.0].groupby([\"decade\",\"Dominant_Topic\"]).size().reset_index()\n",
    "data_by_decade_6 = df_combined[df_combined.Dominant_Topic==6.0].groupby([\"decade\",\"Dominant_Topic\"]).size().reset_index()\n",
    "data_by_decade_7 = df_combined[df_combined.Dominant_Topic==7.0].groupby([\"decade\",\"Dominant_Topic\"]).size().reset_index()\n",
    "data_by_decade_8 = df_combined[df_combined.Dominant_Topic==8.0].groupby([\"decade\",\"Dominant_Topic\"]).size().reset_index()\n",
    "\n",
    "overview_df = pd.concat([data_by_decade_0[0], data_by_decade_1[0],\n",
    "                  data_by_decade_2[0], data_by_decade_3[0],\n",
    "                  data_by_decade_4[0], data_by_decade_5[0],\n",
    "                  data_by_decade_6[0], data_by_decade_7[0],\n",
    "                  data_by_decade_8[0]], axis=1)\n",
    "\n",
    "overview_df_sums=overview_df.sum(axis=1)\n",
    "\n",
    "data_by_decade_0['Topic_0'] = round(data_by_decade_0[0]/overview_df_sums * 100,2)\n",
    "data_by_decade_1['Topic_1'] = round(data_by_decade_1[0]/overview_df_sums * 100,2)\n",
    "data_by_decade_2['Topic_2'] = round(data_by_decade_2[0]/overview_df_sums * 100,2)\n",
    "data_by_decade_3['Topic_3'] = round(data_by_decade_3[0]/overview_df_sums * 100,2)\n",
    "data_by_decade_4['Topic_4'] = round(data_by_decade_4[0]/overview_df_sums * 100,2)\n",
    "data_by_decade_5['Topic_5'] = round(data_by_decade_5[0]/overview_df_sums * 100,2)\n",
    "data_by_decade_6['Topic_6'] = round(data_by_decade_6[0]/overview_df_sums * 100,2)\n",
    "data_by_decade_7['Topic_7'] = round(data_by_decade_7[0]/overview_df_sums * 100,2)\n",
    "data_by_decade_8['Topic_8'] = round(data_by_decade_8[0]/overview_df_sums * 100,2)\n",
    "\n",
    "overview_df = pd.concat([data_by_decade_0['Topic_0'], data_by_decade_1['Topic_1'],\n",
    "                  data_by_decade_2['Topic_2'], data_by_decade_3['Topic_3'],\n",
    "                  data_by_decade_4['Topic_4'], data_by_decade_5['Topic_5'],\n",
    "                  data_by_decade_6['Topic_6'], data_by_decade_7['Topic_7'],\n",
    "                  data_by_decade_8['Topic_8']], axis=1)\n",
    "\n",
    "# show the graph\n",
    "ax = overview_df.plot(stacked=True, kind='bar', figsize=(18, 16))\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "# sets the tick labels so time isn't included\n",
    "ax.xaxis.set_major_formatter(plt.FixedFormatter(data_by_decade['decade']))\n",
    "plt.xticks(rotation=45)\n",
    "ax.set_xlabel(\"Jahrzehnt\",fontsize=20)\n",
    "ax.set_ylabel(\"Prozent\",fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erstellen von prozentualler Übersicht\n",
    "overview_df.index = data_by_decade['decade']\n",
    "overview_df=overview_df.fillna(0)\n",
    "overview_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literaturrrechersche nach MeSH-Codes im Datenbestand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erzeugen von Dataframe und filtern auf PMIDs\n",
    "articles\n",
    "articles_filtered = articles[articles.keywords.str.contains('cytokines',na=False)]\n",
    "articles_filtered.pmid[0:50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
